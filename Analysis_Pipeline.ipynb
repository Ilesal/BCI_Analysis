{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG based Brain-Computer Interface (BCI) using Visual Imagery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Project for Computational Cognitive Neuroscience 2020/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief description of the dataset has been been included in Datasets/Description.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Datasets\n",
    "2. Exclude unwanted channels \n",
    "3. Apply a bandpass FIR filter using Hamming window to the raw signal (between 1Hz-40Hz) \n",
    "4. Create Epochs\n",
    "5. ICA\n",
    "6. Remove bad epochs\n",
    "7. Time-Frequency Analysis\n",
    "6. Temporal Decoding\n",
    "7. Temporal Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture libraries   \n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install mne\n",
    "!{sys.executable} -m pip install mne-features\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import pathlib\n",
    "import mne\n",
    "from mne.io import read_raw_edf\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs,corrmap\n",
    "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch, tfr_stockwell\n",
    "matplotlib.use('Qt5Agg') #allow interactive plots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mne.decoding import SlidingEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mne.decoding import Scaler, Vectorizer, cross_val_multiscore\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import GeneralizingEstimator, Scaler,cross_val_multiscore, LinearModel, get_coef, Vectorizer, CSP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading EEG Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function to load all data files with extensions '.edf':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \n",
    "    '''\n",
    "    Load the .edf datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : the directory path where your data are stored\n",
    " \n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    list_load_dataset : list of .edf files\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    list_files = os.listdir(path=path) #set the directory path\n",
    "    \n",
    "    extension = '.edf'\n",
    "    index = 0\n",
    "    list_dataset = [] #create an empty list to store our .edf files\n",
    "    for file in list_files: #for each file in our directory\n",
    "        if extension in list_files[index]: #if the file's extension is equal to .edf\n",
    "            list_dataset.append(list_files[index]) #add the file in list_dataset\n",
    "        index += 1 \n",
    "\n",
    "    list_load_dataset = []\n",
    "    for n_file in range(0, len(list_dataset)): #for each .edf file in our list \n",
    "        dataset = read_raw_edf(list_dataset[n_file], preload=True) #load the file\n",
    "        list_load_dataset.append(dataset)\n",
    "        \n",
    "    return list_load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "raw_datasets = load_data(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Overall, we have',(len(raw_datasets)), 'experimental sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the information available for a given dataset, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[0].info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The shape of the following dataset:',(raw_datasets[0].get_data().shape), 'indicates that we have', raw_datasets[0].get_data().shape[0],\n",
    "'channels and',raw_datasets[0].get_data().shape[1], 'timepoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually explore the first raw unfiltered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets[0].plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exclude unused channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below iterate thorugh each .edf file and remove the unwanted channels (i.e. the channels excluded from the include_channels list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the channels that you want to include in the analysis:\n",
    "\n",
    "include_channels = ['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4']\n",
    "#reference_channels = ['CQ_CMS', 'CQ_DRL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def excl_chan(data):\n",
    "\n",
    "    ''' \n",
    "    This function exclude the channels we don't need from further analysis. If you want\n",
    "    to add or remove some channels, modify the above list \"include_channels\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list_datasets: the list of datasets with unused channels removed\n",
    "    \n",
    "    '''\n",
    "       \n",
    "    list_datasets=[]     \n",
    "    for n_file in range(0, len(data)):    \n",
    "         for chan_name in data[n_file].ch_names: \n",
    "            if chan_name not in include_channels:\n",
    "                data[n_file].drop_channels([chan_name])   \n",
    "                list_datasets.append(data)\n",
    "                \n",
    "                                                   \n",
    "    return list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "excl_chan(raw_datasets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check the new list of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[0].ch_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Power Spectrum Density (PSD) of the first unfiltered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets[0].plot_psd(average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Filtering datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function iterates through each file and apply a Hamming windowed FIR band-pass filter (default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def filter_data(data):\n",
    "    \n",
    "    '''\n",
    "    This function filter the raw datasets. Because we are interested in low frequencies, \n",
    "    in the alpha-beta frequency range, we can band-pass filter between 1Hz-40Hz.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw continuous unfiltered datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_data: the datasets containing the filtered data.\n",
    "    '''\n",
    "\n",
    "\n",
    "    filtered_data=[]\n",
    "    for file in range(0, len(data)):\n",
    "        #data[file].notch_filter(freqs=(50), filter_length='auto', phase='zero') #uncomment to apply notch filter to remove power-noise\n",
    "        #data[file].filter(0.16, None, fir_design='firwin') #high-pass filter to remove slow drifts\n",
    "        data[file].filter(1., 40., fir_design='firwin')  #apply band-pass filter between 1 and 40 HZ, our freqs range of interest\n",
    "        filtered_data.append(data[file])\n",
    "        \n",
    "    return filtered_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function:\n",
    "\n",
    "filter_data(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect now the PSD of a filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[0].plot_psd(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot the PSD for each session to visually inspect them:\n",
    "\n",
    "#def plot_data(data):   #the input is our filtered dataset\n",
    "#    for file in range(0, len(data)):\n",
    "#        data[file].plot_psd(average=True)   \n",
    "\n",
    "\n",
    "#plot_data(raw_datasets)  #call the function to plot 30 PSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to epoch the continuous data to 9.498 second segments. The first and last 250ms have been removed from each epoch to avoid potential overlapping events in the epoched data. \n",
    "With 30 sessions we will have 300 epochs in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs(list_prep_dataset, duration):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function extract the epochs object from the preprocessed list of datasets. The conditions are:\n",
    "    Push = 1\n",
    "    Relax = 0\n",
    "    --> Note: This function should be called after the preprocessing, but before the ICA <--\n",
    "\n",
    "    :param list_prep_dataset: a list containing the preprocessed datasets\n",
    "    :param duration: the duration of each epochs (10 seconds)\n",
    "    :return: epochs: the mne.Epochs object containing the epoched data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #event_dict = {'Relax': 0, 'Push': 1}   \n",
    "    \n",
    "    list_epochs = []  \n",
    "    for prep_dataset in list_prep_dataset:\n",
    "        events = mne.make_fixed_length_events(prep_dataset, id=0, start=65.0, stop=165.0, duration=10.0) # make fixed-length events for each dataset in list_prep_dateset\n",
    "        \n",
    "        for n_events in range(0, len(events)):\n",
    "            if n_events % 2 == 1: \n",
    "                events[n_events][2] = 1 \n",
    "                \n",
    "          \n",
    "        # make epochs for each dataset\n",
    "        epochs = mne.Epochs(prep_dataset, events, tmin=0.0, tmax=9.998, event_id=event_dict, baseline=(0, 0), preload=True)\n",
    "        list_epochs.append(epochs)\n",
    "                \n",
    "        # combine epochs\n",
    "        epochs = mne.concatenate_epochs(list_epochs) \n",
    "\n",
    "        # crop start and end of the epochs based on provided time reference\n",
    "        epochs.crop(tmin=0.25, tmax=9.998 - 0.25) \n",
    "        #epochs.crop(tmin=, tmax=9.998 - 6.25)  #I tried also with: #(tmax=9.998 - 2.75) #(t_max=9.998 - 6.75) # (t_max=9.998 - 7.25)\n",
    "\n",
    "        # Generate Standard montage (useful for ICA and TimeFrequency analyses)\n",
    "        biosemi_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "        epochs.set_montage(biosemi_montage) \n",
    "        \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "epoched_data=make_epochs(raw_datasets, 10)  \n",
    "epoched_data_copy=make_epochs(raw_datasets, 10) #create a copy that can be used to compare before and after epochs rejections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(epoched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the PSD for the two conditions: Relax and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data['Relax'].plot_psd()\n",
    "epoched_data['Push'].plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evoked responses by averaging each epochs' conditions and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data['Relax'].average().plot(titles='Relax Condition') #it will show a butterfly plot of each channel type\n",
    "epoched_data['Push'].average().plot(titles='Push Condition')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply Independent Component Analysis (ICA) to remove artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply ICA to all epochs\n",
    "\n",
    "picks = raw_datasets[0].info['ch_names'] #define the channels we want to include in the analysis\n",
    "ica=ICA(n_components=14, method='fastica', max_iter=1000, random_state=89) #define the parameters\n",
    "ica.fit(epoched_data,  picks = picks, reject = dict(eeg = 200e-6)) #apply ICA to epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_components(picks=range(14), inst=epoched_data)  #plot the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting each components individually, explore also their time course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_sources(epoched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the components to remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude=[0,1,13] #exclude eye movements and heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.apply(epoched_data, exclude=ica.exclude) #exclude the two components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reject Bad Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before rejecting epochs based on the peak-to-peak (PTP) amplitude, let's investigate on which temporal segment the \n",
    "PTP is most likely to exceed the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, let's explore the ninth epoch:\n",
    "epo=epoched_data.get_data()[9] #try also epoch n 18,50,51,52,76..\n",
    "\n",
    "\n",
    "ampl=[] #create an empty list to store all the PTP amplitudes. With 14 channels, we will have 140 values.\n",
    "idx=[] #create a list to add the epochs' index  \n",
    "\n",
    "window_size = 243\n",
    "\n",
    "for n_ch in range(0,len(epo)): #for each channel \n",
    "    i = 0 #starting index\n",
    "    numbers = epo[n_ch][:] #select all the timepoints of that channel\n",
    "    while i < len(numbers) - window_size +1:       \n",
    "        window_amplitude = np.max(numbers[i : i + window_size])-np.min(numbers[i : i + window_size]) #select your window of interest and compute PTP\n",
    "        ampl.append(window_amplitude) #add the PTP value to 'ampl'\n",
    "        idx.append(i)\n",
    "        i += 243\n",
    "        \n",
    "        if i > len(numbers) - window_size +1:  #move to the next channel \n",
    "               n_ch += 1\n",
    "\n",
    "print(len(ampl)) #check you have 140 PTP values (i.e. 10 PTP values for each channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, define your PTP threshold \n",
    "\n",
    "thr=200e-6 #200uV\n",
    "bad_segments=np.argwhere(np.array(ampl)>thr)\n",
    "print(bad_segments) #check where the amplitude exceed the threshold and use these values to plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the t_start and t_end points for plotting:\n",
    "\n",
    "t_start=idx[26] #use the bad_segments value to retrieve the corresponding idx and plotting the segment\n",
    "t_end=t_start+243 #define the end of the bad segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot them\n",
    "plt.plot(epo[0,t_start:t_end]) #select the first channel\n",
    "plt.xlabel('Samples') #243 samples correspond to 10 seconds\n",
    "plt.ylabel('uV')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having explored which individual segments exceed a given value, we can continue defining the rejection peak-to-peak (PTP) amplitude threshold to 200uV (this threshold will reject 10% of bad epochs) using MNE tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_criteria = dict(eeg=200e-6) #PTP threshold\n",
    "flat_criteria = dict(eeg=1e-6) # 1 µV, minimum acceptable peak-to-peak amplitudes\n",
    "epoched_data.drop_bad(reject=reject_criteria, flat=flat_criteria) \n",
    "\n",
    "print(epoched_data.get_data().shape) #print the new data shape\n",
    "print(epoched_data.drop_log) #print the total number of epochs rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data.plot_drop_log() #plot the percentage of epochs rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Time-Frequencies Analysis (TFR) and Inter-Trial Coherence (ITC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute time-frequency representations (TFRs) from our epoched data:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.logspace(*np.log10([4, 30]), num=40) # define frequencies of interest (log-spaced) \n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "\n",
    "#Compute power and ITC for RELAX condition\n",
    "power_r, itc_r = mne.time_frequency.tfr_morlet(epoched_data['Relax'], freqs=freqs, n_cycles=n_cycles, \n",
    "                                           use_fft=True, average=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "\n",
    "#Compute power and ITC for PUSH condition\n",
    "power_p, itc_p = mne.time_frequency.tfr_morlet(epoched_data['Push'], freqs=freqs, n_cycles=n_cycles, \n",
    "                                           use_fft=True, average=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Power average across epochs in the Alpha and Beta band frequencies for each condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## RELAX #############\n",
    "\n",
    "#Alpha band\n",
    "\n",
    "relax_pow_a = [] #store the power average for each channel  \n",
    "\n",
    "for a_file in range(0, len(raw_datasets[0].ch_names)): #for each file in the range 0-14\n",
    "    pow_ar = power_r.data[a_file][(power_r.freqs>=8) & (power_r.freqs<=12)] #select the alpha band freqs range\n",
    "    pow_avg_ar = np.mean(pow_ar, axis=0) #compute the average\n",
    "    relax_pow_a.append(pow_avg_ar)  \n",
    "    \n",
    "    \n",
    "#Beta band\n",
    "\n",
    "relax_pow_b = [] #store the power average for each channel  \n",
    "\n",
    "for b_file in range(0, len(raw_datasets[0].ch_names)): #for each file in the range 0-14\n",
    "    pow_br = power_r.data[b_file][(power_r.freqs>12) & (power_r.freqs<=30)] #select the beta band freqs range\n",
    "    pow_avg_br = np.mean(pow_br, axis=0) #compute the average\n",
    "    relax_pow_b.append(pow_avg_br)  \n",
    "    \n",
    "    \n",
    "#Theta band\n",
    "\n",
    "relax_pow_t = [] #store the power average for each channel  \n",
    "\n",
    "for c_file in range(0, len(raw_datasets[0].ch_names)): #for each file in the range 0-14\n",
    "    pow_tr = power_r.data[c_file][(power_r.freqs>=4) & (power_r.freqs<8)] #select the beta band freqs range\n",
    "    pow_avg_tr = np.mean(pow_tr, axis=0) #compute the average\n",
    "    relax_pow_t.append(pow_avg_tr)  \n",
    "    \n",
    "\n",
    "    \n",
    "######################### PUSH ###########\n",
    "\n",
    "#Alpha band\n",
    "\n",
    "push_pow_a = []\n",
    "\n",
    "for d_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    pow_ap = power_p.data[d_file][(power_p.freqs>=8) & (power_p.freqs<=12)] \n",
    "    pow_avg_ap = np.mean(pow_ap, axis=0)\n",
    "    push_pow_a.append(pow_avg_ap)\n",
    "    \n",
    "    \n",
    "#Beta band\n",
    "\n",
    "push_pow_b = []\n",
    "\n",
    "for e_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    pow_bp = power_p.data[e_file][(power_p.freqs>12) & (power_p.freqs<=30)] \n",
    "    pow_avg_bp = np.mean(pow_bp, axis=0)\n",
    "    push_pow_b.append(pow_avg_bp)\n",
    "    \n",
    "    \n",
    "#Theta band\n",
    "\n",
    "push_pow_t = []\n",
    "\n",
    "for f_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    pow_tp = power_p.data[f_file][(power_p.freqs>=4) & (power_p.freqs<8)] \n",
    "    pow_avg_tp = np.mean(pow_tp, axis=0)\n",
    "    push_pow_t.append(pow_avg_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the lines below to double-check if the right frequency range is being selected:\n",
    "\n",
    "#print(power_r.freqs[np.argwhere((power_r.freqs>=8) & (power_r.freqs<=12)).flatten()]) #alpha band\n",
    "#print(power_r.freqs[np.argwhere((power_r.freqs>12) & (power_r.freqs<=30)).flatten()]) #beta band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the power difference between Relax vs Push condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4'] #reminder of our channels idx\n",
    "\n",
    "#Alpha Band\n",
    "\n",
    "plt.plot(relax_pow_a[2]) \n",
    "plt.plot(push_pow_a[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('Power[db]')\n",
    "plt.title('Channel F3 - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(relax_pow_a[6]) \n",
    "plt.plot(push_pow_a[6]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('Power[db]')\n",
    "plt.title('Channel O1 - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Beta Band\n",
    "\n",
    "plt.plot(relax_pow_b[2])  \n",
    "plt.plot(push_pow_b[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('Power[db]')\n",
    "plt.title('Channel F3 - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(relax_pow_b[6]) \n",
    "plt.plot(push_pow_b[6]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('Power[db]')\n",
    "plt.title('Channel O1 - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "#Theta Band\n",
    "plt.plot(relax_pow_t[2]) \n",
    "plt.plot(push_pow_t[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('Power[db]')\n",
    "plt.title('Channel F3 - Theta Band')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ITC average for Alpha and Beta band for each channel and both conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## RELAX #############\n",
    "\n",
    "#Alpha band\n",
    "\n",
    "relax_itc_a  = []\n",
    "\n",
    "for a_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_alphar = itc_r.data[a_file][(itc_r.freqs>=8) & (itc_r.freqs<=12)] \n",
    "    itc_alphar_avg = np.mean(itc_alphar, axis=0)\n",
    "    relax_itc_a.append(itc_alphar_avg)\n",
    "    \n",
    "    \n",
    "#Beta band\n",
    "\n",
    "relax_itc_b  = []\n",
    "\n",
    "for b_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_betar = itc_r.data[b_file][(itc_r.freqs>12) & (itc_r.freqs<=30)] \n",
    "    itc_betar_avg = np.mean(itc_betar, axis=0)\n",
    "    relax_itc_b.append(itc_betar_avg)\n",
    "    \n",
    "\n",
    "#Theta band\n",
    "\n",
    "relax_itc_t  = []\n",
    "\n",
    "for c_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_thetar = itc_r.data[c_file][(itc_r.freqs>=4) & (itc_r.freqs<8)] \n",
    "    itc_thetar_avg = np.mean(itc_thetar, axis=0)\n",
    "    relax_itc_t.append(itc_thetar_avg)\n",
    "    \n",
    "    \n",
    "########################## PUSH #############\n",
    "    \n",
    "#Alpha band\n",
    "\n",
    "\n",
    "push_itc_a = []\n",
    "\n",
    "for d_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_alphap=itc_p.data[d_file][(itc_p.freqs>=8) & (itc_p.freqs<=12)] \n",
    "    itc_alphap_avg=np.mean(itc_alphap, axis=0)\n",
    "    push_itc_a.append(itc_alphap_avg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#Beta band\n",
    "\n",
    "\n",
    "push_itc_b = []\n",
    "\n",
    "for e_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_betap=itc_p.data[e_file][(itc_p.freqs>12) & (itc_p.freqs<=30)] \n",
    "    itc_betap_avg=np.mean(itc_betap, axis=0)\n",
    "    push_itc_b.append(itc_betap_avg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#Theta band\n",
    "\n",
    "\n",
    "push_itc_t = []\n",
    "\n",
    "for f_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_thetap=itc_p.data[f_file][(itc_p.freqs>=4) & (itc_p.freqs<8)] \n",
    "    itc_thetap_avg=np.mean(itc_thetap, axis=0)\n",
    "    push_itc_t.append(itc_thetap_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ITC for a given channel, relax vs push:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha band\n",
    "\n",
    "plt.plot(relax_itc_a[2]) \n",
    "plt.plot(push_itc_a[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Beta band\n",
    "\n",
    "plt.plot(relax_itc_b[2]) \n",
    "plt.plot(push_itc_b[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Beta Band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the ITC for the same condition across channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### RELAX ##########   \n",
    "\n",
    "#ALPHA band\n",
    "    \n",
    "plt.plot(relax_itc_a[3]) \n",
    "plt.plot(relax_itc_a[7]) \n",
    "plt.legend([\"Relax FC5\", \"Relax O2\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(relax_itc_a[2]) \n",
    "plt.plot(relax_itc_a[6]) \n",
    "plt.legend([\"Relax F3\", \"Relax O1\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#BETA band\n",
    "    \n",
    "plt.plot(relax_itc_b[3]) \n",
    "plt.plot(relax_itc_b[7]) \n",
    "plt.legend([\"Relax FC5\", \"Relax O2\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(relax_itc_b[2]) \n",
    "plt.plot(relax_itc_b[6]) \n",
    "plt.legend([\"Relax F3\", \"Relax O1\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "#THETA band \n",
    "\n",
    "plt.plot(relax_itc_t[3]) \n",
    "plt.plot(relax_itc_t[5]) \n",
    "plt.legend([\"Relax FC5\", \"Relax P7\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Theta Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(relax_itc_t[3]) \n",
    "plt.plot(relax_itc_t[5]) \n",
    "plt.legend([\"Relax F3\", \"Relax P7\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Theta Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####### PUSH ##########\n",
    "#ALPHA band \n",
    "    \n",
    "plt.plot(push_itc_a[3]) \n",
    "plt.plot(push_itc_a[7]) \n",
    "plt.legend([\"Push FC5\", \"Push O2\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(push_itc_a[2]) \n",
    "plt.plot(push_itc_a[6]) \n",
    "plt.legend([\"Push F3\", \"Push O1\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#BETA band\n",
    "    \n",
    "plt.plot(push_itc_b[3]) \n",
    "plt.plot(push_itc_b[7]) \n",
    "plt.legend([\"Relax FC5\", \"Relax O2\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(push_itc_b[2]) \n",
    "plt.plot(push_itc_b[6]) \n",
    "plt.legend([\"Relax F3\", \"Relax O1\"])\n",
    "plt.xlabel('Time[s]')\n",
    "plt.ylabel('ITC')\n",
    "plt.title('ITC  - Beta Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#THETA band \n",
    "\n",
    "plt.plot(push_itc_t[2]) \n",
    "plt.plot(push_itc_t[6]) \n",
    "plt.legend([\"Push F3\", \"Push O1\"])\n",
    "plt.title('ITC  - Theta Band')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Pattern Analysis (MVPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Decoding over time \n",
    "#### Fit the classifier at every single time point to see at which time points  it can discriminate between the two conditions (through Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y.\n",
    "\n",
    "X = epoched_data.get_data()\n",
    "y = epoched_data.events[:, 2]\n",
    "\n",
    "# Classifier pipeline. \n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    LogisticRegression())\n",
    "\n",
    "scoring = 'roc_auc'\n",
    "time_decoder = SlidingEstimator(clf, scoring=scoring, n_jobs=1, verbose=True) # The \"sliding estimator\" will train the classifier at each time point.\n",
    "\n",
    "# Run cross-validation.\n",
    "n_splits = 5\n",
    "scores = cross_val_multiscore(time_decoder, X, y, cv=5, n_jobs=1)\n",
    "\n",
    "# Mean scores across cross-validation splits, for each time point.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "\n",
    "# Mean score across all time points.\n",
    "mean_across_all_times = round(np.mean(scores), 3)\n",
    "print(f'\\n=> Mean CV score across all time points: {mean_across_all_times:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.axhline(0.5, color='k', linestyle='--', label='chance')  # AUC = 0.5\n",
    "ax.axvline(0, color='k', linestyle='-')  # Mark time point zero.\n",
    "ax.plot(epoched_data.times, mean_scores, label='score')\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Mean ROC AUC')\n",
    "ax.legend()\n",
    "ax.set_title('Relax vs Push')\n",
    "fig.suptitle('Sensor Space Decoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Temporal generalization¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate whether the model estimated at a particular time instant accurately predicts any other time instant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = epoched_data.get_data()\n",
    "y = epoched_data.events[:, 2]\n",
    "\n",
    "# define the Temporal generalization object\n",
    "time_gen = GeneralizingEstimator(clf, n_jobs=1, scoring='roc_auc',\n",
    "                                 verbose=True)\n",
    "\n",
    "scores = cross_val_multiscore(time_gen, X, y, cv=5, n_jobs=1)\n",
    "\n",
    "# Mean scores across cross-validation splits\n",
    "scores = np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the diagonal (it's exactly the same as the time-by-time decoding above)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epoched_data.times, np.diag(scores), label='score')\n",
    "ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "ax.set_xlabel('Times')\n",
    "ax.set_ylabel('AUC')\n",
    "ax.legend()\n",
    "ax.axvline(.0, color='k', linestyle='-')\n",
    "ax.set_title('Decoding EEG sensors over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the full (generalization) matrix:\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(scores, interpolation='lanczos', origin='lower', cmap='RdBu_r',\n",
    "               extent=epoched_data.times[[0, -1, 0, -1]], vmin=0., vmax=4.)\n",
    "ax.set_xlabel('Testing Time (s)')\n",
    "ax.set_ylabel('Training Time (s)')\n",
    "ax.set_title('Temporal generalization')\n",
    "ax.axvline(0, color='k')\n",
    "ax.axhline(0, color='k')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create a continuos epoch:\n",
    "\n",
    "x=len(raw_datasets[0])\n",
    "dur=x/256\n",
    "epoch= mne.make_fixed_length_epochs(raw_datasets[0], duration=dur,  preload=True)\n",
    "epoch.crop(tmin=65.0, tmax=165.0)\n",
    "print(np.shape(epoch))\n",
    "\n",
    "biosemi_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "epoch.set_montage(biosemi_montage)\n",
    "\n",
    "\n",
    "#Apply ICA to the epoch\n",
    "\n",
    "picks = raw_datasets[0].info['ch_names'] #define the channels we want to include in the analysis\n",
    "ica=ICA(n_components=14, method='fastica', max_iter=1000, random_state=89) #define the parameters\n",
    "ica.fit(epoch,  picks = picks, reject = dict(eeg = 200e-6)) #apply ICA to epochs\n",
    "\n",
    "#ica.plot_components(picks=range(14), inst=epoch)  #plot the components\n",
    "\n",
    "ica.exclude=[0,1,13] #exclude eye movements and heartbeat\n",
    "\n",
    "ica.apply(epoch, exclude=ica.exclude) #exclude the two components\n",
    "\n",
    "#Reject bad epochs:\n",
    "\n",
    "reject_criteria = dict(eeg=200e-6) #PTP threshold\n",
    "flat_criteria = dict(eeg=1e-6) # 1 µV, minimum acceptable peak-to-peak amplitudes\n",
    "epoch.drop_bad(reject=reject_criteria, flat=flat_criteria) \n",
    "\n",
    "print(epoch.get_data().shape) #print the new data shape\n",
    "print(epoch.drop_log) #print the total number of epochs rejected\n",
    "\n",
    "#TFR\n",
    "\n",
    "freqs = np.logspace(*np.log10([4, 30]), num=40) # define frequencies of interest (log-spaced) \n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "\n",
    "#Compute power  \n",
    "power = mne.time_frequency.tfr_morlet(epoch, freqs=freqs, n_cycles=n_cycles, \n",
    "                                           use_fft=True, average=False,\n",
    "                                           return_itc=False, decim=3, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "#average\n",
    "epo_pow = [] #store the power average for each channel  \n",
    "\n",
    "for chan in range(0, len(raw_datasets[0].ch_names)):\n",
    "    epo_p = (power.data[0][chan][(power.freqs>=8) & (power.freqs<=12)])\n",
    "    epo_avg = np.mean(epo_p, axis=0) #compute the average\n",
    "    epo_pow.append(epo_avg) \n",
    "\n",
    "    \n",
    "#Plot\n",
    "\n",
    "plt.plot(epo_pow[6]) \n",
    "#plt.xlabel('Time[s]')\n",
    "#plt.ylabel('Power[db]')\n",
    "#plt.title('Channel O1  - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "#np.shape(power.data[0][0][(power.freqs>=8) & (power.freqs<=12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
