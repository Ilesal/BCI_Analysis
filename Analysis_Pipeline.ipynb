{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG based Brain-Computer Interface (BCI) using Visual Imagery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Project for Computational Cognitive Neuroscience 2020/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief description of the dataset has been been included in Datasets/Description.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture libraries   \n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install mne\n",
    "!{sys.executable} -m pip install mne-features\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import pathlib\n",
    "import mne\n",
    "from mne.io import read_raw_edf\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs,corrmap\n",
    "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch, tfr_stockwell\n",
    "matplotlib.use('Qt5Agg') #allow interactive plots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mne.decoding import SlidingEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mne.decoding import Scaler, Vectorizer, cross_val_multiscore\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EEG Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function to load all data files with extensions '.edf':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \n",
    "    '''\n",
    "    Load the .edf datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : the directory path where your data are stored\n",
    " \n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    list_load_dataset : list of .edf files\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    list_files = os.listdir(path=path) #set the directory path\n",
    "    \n",
    "    extension = '.edf'\n",
    "    index = 0\n",
    "    list_dataset = [] #create an empty list to store our .edf files\n",
    "    for file in list_files: #for each file in our directory\n",
    "        if extension in list_files[index]: #if the file's extension is equal to .edf\n",
    "            list_dataset.append(list_files[index]) #add the file in list_dataset\n",
    "        index += 1 \n",
    "\n",
    "    list_load_dataset = []\n",
    "    for n_file in range(0, len(list_dataset)): #for each .edf file in our list \n",
    "        dataset = read_raw_edf(list_dataset[n_file], preload=True) #load the file\n",
    "        list_load_dataset.append(dataset)\n",
    "        \n",
    "    return list_load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "raw_datasets = load_data(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Overall, we have',(len(raw_datasets)), 'experimental sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the information available for a given dataset, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[0].info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The shape of the following dataset:',(raw_datasets[0].get_data().shape), 'indicates that we have', raw_datasets[0].get_data().shape[0],\n",
    "'channels and',raw_datasets[0].get_data().shape[1], 'timepoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually explore the first raw unfiltered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets[0].plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude unused channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below iterate thorugh each .edf file and remove the unwanted channels (i.e. the channels excluded from the include_channels list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the channels that you want to include in the analysis:\n",
    "\n",
    "include_channels = ['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4']\n",
    "#reference_channels = ['CQ_CMS', 'CQ_DRL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def excl_chan(data):\n",
    "\n",
    "    ''' \n",
    "    This function exclude the channels we don't need from further analysis. If you want\n",
    "    to add or remove some channels, modify the above list \"include_channels\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list_datasets: the list of datasets with unused channels removed\n",
    "    \n",
    "    '''\n",
    "       \n",
    "    list_datasets=[]     \n",
    "    for n_file in range(0, len(data)):    \n",
    "         for chan_name in data[n_file].ch_names: \n",
    "            if chan_name not in include_channels:\n",
    "                data[n_file].drop_channels([chan_name])   \n",
    "                list_datasets.append(data)\n",
    "                \n",
    "                                                   \n",
    "    return list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "excl_chan(raw_datasets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check the new list of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[0].ch_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Power Spectrum Density (PSD) of the first unfiltered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets[0].plot_psd(average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Filtering datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function iterates through each file and apply a Hamming windowed FIR band-pass filter (default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def filter_data(data):\n",
    "    \n",
    "    '''\n",
    "    This function filter the raw datasets. Because we are interested in low frequencies, \n",
    "    in the alpha-beta frequency range, we can band-pass filter between 1Hz-40Hz.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw continuous unfiltered datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_data: the datasets containing the filtered data.\n",
    "    '''\n",
    "\n",
    "\n",
    "    filtered_data=[]\n",
    "    for file in range(0, len(data)):\n",
    "        #data[file].notch_filter(freqs=(50), filter_length='auto', phase='zero') #uncomment to apply notch filter to remove power-noise\n",
    "        data[file].filter(1., 40., fir_design='firwin')  \n",
    "        filtered_data.append(data[file])\n",
    "        \n",
    "    return filtered_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function:\n",
    "\n",
    "filter_data(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect now the PSD of a filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[0].plot_psd(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot the PSD for each session to visually inspect them:\n",
    "\n",
    "#def plot_data(data):   #the input is our filtered dataset\n",
    "#    for file in range(0, len(data)):\n",
    "#        data[file].plot_psd(average=True)   \n",
    "\n",
    "\n",
    "#plot_data(raw_datasets)  #call the function to plot 30 PSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to epoch the continuous data to 9.498 second segments. The first and last 250ms have been removed from each epoch to avoid potential overlapping events in the epoched data. \n",
    "With 30 sessions we will have 300 epochs in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs(list_prep_dataset, duration):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function extract the epochs object from the preprocessed list of datasets. The conditions are:\n",
    "    Push = 1\n",
    "    Relax = 0\n",
    "    --> Note: This function should be called after the preprocessing, but before the ICA <--\n",
    "\n",
    "    :param list_prep_dataset: a list containing the preprocessed datasets\n",
    "    :param duration: the duration of each epochs (10 seconds)\n",
    "    :return: epochs: the mne.Epochs object containing the epoched data.\n",
    "    \"\"\"\n",
    "    \n",
    "    event_dict = {'Relax': 0, 'Push': 1}   \n",
    "    \n",
    "    list_epochs = []  \n",
    "    for prep_dataset in list_prep_dataset:\n",
    "        events = mne.make_fixed_length_events(prep_dataset, id=0, start=65.0, stop=165.0, duration=duration) # make fixed-length events for each dataset in list_prep_dateset\n",
    "        \n",
    "        for n_events in range(0, len(events)):\n",
    "            if n_events % 2 == 1: \n",
    "                events[n_events][2] = 1 \n",
    "                \n",
    "          \n",
    "        # make epochs for each dataset\n",
    "        epochs = mne.Epochs(prep_dataset, events, tmin=0.0, tmax=9.998, event_id=event_dict, baseline=(0, 0), preload=True)\n",
    "        list_epochs.append(epochs)\n",
    "                \n",
    "        # combine epochs\n",
    "        epochs = mne.concatenate_epochs(list_epochs) \n",
    "\n",
    "        # crop start and end of the epochs based on provided time reference\n",
    "        epochs.crop(tmin=0.25, tmax=9.998 - 0.25) \n",
    "\n",
    "        # Generate Standard montage (useful for ICA and TimeFrequency analyses)\n",
    "        biosemi_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "        epochs.set_montage(biosemi_montage) \n",
    "        \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function\n",
    "\n",
    "epoched_data=make_epochs(raw_datasets, 10)  \n",
    "epoched_data_copy=make_epochs(raw_datasets, 10) #create a copy that can be used to compare before and after epochs rejections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the PSD for the two conditions: Relax and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data['Relax'].plot_psd()\n",
    "epoched_data['Push'].plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evoked responses by averaging each epochs' conditions and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data['Relax'].average().plot(titles='Relax Condition') #it will show a butterfly plot of each channel type\n",
    "epoched_data['Push'].average().plot(titles='Push Condition')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Independent Component Analysis (ICA) to remove artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply ICA to all epochs\n",
    "\n",
    "picks = raw_datasets[0].info['ch_names'] #define the channels we want to include in the analysis\n",
    "ica=ICA(n_components=14, method='fastica', max_iter=1000, random_state=89) #define the parameters\n",
    "ica.fit(epoched_data,  picks = picks, reject = dict(eeg = 200e-6)) #apply ICA to epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_components(picks=range(14), inst=epoched_data)  #plot the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting each components individually, explore also their time course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_sources(epoched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the components to remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude=[0,1,13] #exclude eye movements and heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.apply(epoched_data, exclude=ica.exclude) #exclude the two components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reject Bad Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before rejecting epochs based on the peak-to-peak (PTP) amplitude, let's investigate on which temporal segment the \n",
    "PTP is most likely to exceed the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, let's explore the ninth epoch:\n",
    "epo=epoched_data.get_data()[52] #try also epoch n 18,50,51,52,76..\n",
    "\n",
    "\n",
    "ampl=[] #create an empty list to store all the PTP amplitudes. With 14 channels, we will have 140 values.\n",
    "\n",
    "window_size = 243\n",
    "\n",
    "for n_ch in range(0,len(epo)): #for each channel \n",
    "    i = 0 #starting index\n",
    "    numbers = epo[n_ch][:] #select all the timepoints of that channel\n",
    "    while i < len(numbers) - window_size +1:       \n",
    "        window_amplitude = np.max(numbers[i : i + window_size])-np.min(numbers[i : i + window_size]) #select your window of interest and compute PTP\n",
    "        ampl.append(window_amplitude) #add the PTP value to 'ampl'\n",
    "        i += 243\n",
    "        \n",
    "        if i > len(numbers) - window_size +1:  #move to the next channel \n",
    "               n_ch += 1\n",
    "\n",
    "print(len(ampl)) #check you have 140 PTP values (i.e. 10 PTP values for each channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, define your PTP threshold \n",
    "\n",
    "thr=200e-6 #200uV\n",
    "np.argwhere(np.array(ampl)>thr) #check where the amplitude exceed the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having explored which individual segments exceed a given value, we can continue defining the rejection peak-to-peak (PTP) amplitude threshold to 200uV (this threshold will reject 10% of bad epochs) using MNE tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_criteria = dict(eeg=200e-6) #PTP threshold\n",
    "flat_criteria = dict(eeg=1e-6) # 1 µV, minimum acceptable peak-to-peak amplitudes\n",
    "epoched_data.drop_bad(reject=reject_criteria, flat=flat_criteria) \n",
    "\n",
    "print(epoched_data.get_data().shape) #print the new data shape\n",
    "print(epoched_data.drop_log) #print the total number of epochs rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data.plot_drop_log() #plot the percentage of epochs rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Frequencies Analysis (TFR) and Inter-Trial Coherence (ITC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute time-frequency representations (TFRs) from our epoched data:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.logspace(*np.log10([7, 30]), num=40) # define frequencies of interest (log-spaced) \n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "\n",
    "#Compute power and ITC for RELAX condition\n",
    "power_r, itc_r = mne.time_frequency.tfr_morlet(epoched_data['Relax'], freqs=freqs, n_cycles=n_cycles, \n",
    "                                           use_fft=True, average=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)\n",
    "\n",
    "#Compute power and ITC for PUSH condition\n",
    "power_p, itc_p = mne.time_frequency.tfr_morlet(epoched_data['Push'], freqs=freqs, n_cycles=n_cycles, \n",
    "                                           use_fft=True, average=True,\n",
    "                                           return_itc=True, decim=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Power average across epochs in the Alpha and Beta band frequencies for each condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELAX Condition - Alpha band\n",
    "\n",
    "relax_pow_a = [] #store the power average for each channel  \n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)): #for each file in the range 0-14\n",
    "    pow_ar = power_r.data[n_file][(power_r.freqs>=8) & (power_r.freqs<=12)] #select the alpha band freqs range\n",
    "    pow_avg_ar = np.mean(pow_ar, axis=0) #compute the average\n",
    "    relax_pow_a.append(pow_avg_ar)  \n",
    "    \n",
    "    \n",
    "#RELAX Condition - Beta band\n",
    "\n",
    "relax_pow_b = [] #store the power average for each channel  \n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)): #for each file in the range 0-14\n",
    "    pow_br = power_r.data[n_file][(power_r.freqs>12) & (power_r.freqs<=30)] #select the beta band freqs range\n",
    "    pow_avg_br = np.mean(pow_br, axis=0) #compute the average\n",
    "    relax_pow_b.append(pow_avg_br)  \n",
    "    \n",
    "    \n",
    "\n",
    "#PUSH Condition - Alpha band\n",
    "\n",
    "push_pow_a = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    pow_ap = power_p.data[n_file][(power_p.freqs>=8) & (power_p.freqs<=12)] \n",
    "    pow_avg_ap = np.mean(pow_ap, axis=0)\n",
    "    push_pow_a.append(pow_avg_ap)\n",
    "    \n",
    "    \n",
    "#PUSH Condition - Beta band\n",
    "\n",
    "push_pow_b = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    pow_bp = power_p.data[n_file][(power_p.freqs>12) & (power_p.freqs<=30)] \n",
    "    pow_avg_bp = np.mean(pow_bp, axis=0)\n",
    "    push_pow_b.append(pow_avg_bp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the lines below to double-check if the right frequencyo range is being selected:\n",
    "\n",
    "#print(power_r.freqs[np.argwhere((itc_r.freqs>=8) & (itc_r.freqs<=12)).flatten()]) #alpha band\n",
    "#print(power_r.freqs[np.argwhere((itc_r.freqs>12) & (itc_r.freqs<=30)).flatten()]) #beta band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the power difference between Relax vs Push condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4'] #reminder of our channels idx\n",
    "\n",
    "#Alpha Band\n",
    "\n",
    "plt.plot(relax_pow_a[2]) \n",
    "plt.plot(push_pow_a[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "#Beta Band\n",
    "plt.plot(relax_pow_b[2]) \n",
    "plt.plot(push_pow_b[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Beta Band')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ITC average for Alpha and Beta band for each channel and both conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELAX Condition - Alpha band\n",
    "\n",
    "relax_itc_a  = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_alphar = itc_r.data[n_file][(itc_r.freqs>=8) & (itc_r.freqs<=12)] \n",
    "    itc_alphar_avg = np.mean(itc_alphar, axis=0)\n",
    "    relax_itc_a.append(itc_alphar_avg)\n",
    "    \n",
    "    \n",
    "#RELAX Condition - Beta band\n",
    "\n",
    "relax_itc_b  = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_betar = itc_r.data[n_file][(itc_r.freqs>12) & (itc_r.freqs<=30)] \n",
    "    itc_betar_avg = np.mean(itc_betar, axis=0)\n",
    "    relax_itc_b.append(itc_betar_avg)\n",
    "    \n",
    "    \n",
    "#Push Condition - Alpha band\n",
    "\n",
    "\n",
    "push_itc_a = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_alphap=itc_p.data[n_file][(itc_p.freqs>=8) & (itc_p.freqs<=12)] \n",
    "    itc_alphap_avg=np.mean(itc_alphap, axis=0)\n",
    "    push_itc_a.append(itc_alphap_avg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#Push Condition - Beta band\n",
    "\n",
    "\n",
    "push_itc_b = []\n",
    "\n",
    "for n_file in range(0, len(raw_datasets[0].ch_names)):\n",
    "    itc_betap=itc_p.data[n_file][(itc_p.freqs>12) & (itc_p.freqs<=30)] \n",
    "    itc_betap_avg=np.mean(itc_betap, axis=0)\n",
    "    push_itc_b.append(itc_betap_avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ITC for a given channel, relax vs push:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha band\n",
    "\n",
    "plt.plot(relax_itc_a[2]) \n",
    "plt.plot(push_itc_a[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Alpha Band')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Beta band\n",
    "\n",
    "plt.plot(relax_itc_b[2]) \n",
    "plt.plot(push_itc_b[2]) \n",
    "plt.legend([\"Relax\", \"Push\"])\n",
    "plt.title('Channel F3 - Alpha Band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding over time \n",
    "#### Fit the classifier at every single time point to see at which time points  it can discriminate between the two conditions (through Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y.\n",
    "\n",
    "X = epoched_data.get_data()\n",
    "y = epoched_data.events[:, 2]\n",
    "\n",
    "# Classifier pipeline. \n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    LogisticRegression())\n",
    "\n",
    "scoring = 'roc_auc'\n",
    "time_decoder = SlidingEstimator(clf, scoring=scoring, n_jobs=1, verbose=True) # The \"sliding estimator\" will train the classifier at each time point.\n",
    "\n",
    "# Run cross-validation.\n",
    "n_splits = 5\n",
    "scores = cross_val_multiscore(time_decoder, X, y, cv=5, n_jobs=1)\n",
    "\n",
    "# Mean scores across cross-validation splits, for each time point.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "\n",
    "# Mean score across all time points.\n",
    "mean_across_all_times = round(np.mean(scores), 3)\n",
    "print(f'\\n=> Mean CV score across all time points: {mean_across_all_times:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.axhline(0.5, color='k', linestyle='--', label='chance')  # AUC = 0.5\n",
    "ax.axvline(0, color='k', linestyle='-')  # Mark time point zero.\n",
    "ax.plot(epoched_data.times, mean_scores, label='score')\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Mean ROC AUC')\n",
    "ax.legend()\n",
    "ax.set_title('Relax vs Push')\n",
    "fig.suptitle('Sensor Space Decoding')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
