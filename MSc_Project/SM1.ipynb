{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID Number: 33385806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-based BCI using Emotiv Epoc X and Visual Imagery: An Exploratory Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Project for Computational Cognitive Neuroscience 2020/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook I have collected the main functions to carry the out EEG Pre-processing Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function to load the EEG datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function to load all data files with extensions '.edf':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  \n",
    "    '''\n",
    "    Load the .edf datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : the directory path where your data are stored\n",
    " \n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    list_load_dataset : list of .edf files\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    list_files = os.listdir(path=path) #set the directory path\n",
    "    \n",
    "    extension = '.edf'\n",
    "    index = 0\n",
    "    list_dataset = [] #create an empty list to store our .edf files\n",
    "    for file in list_files: #for each file in our directory\n",
    "        if extension in list_files[index]: #if the file's extension is equal to .edf\n",
    "            list_dataset.append(list_files[index]) #add the file in list_dataset\n",
    "        index += 1 \n",
    "\n",
    "    list_load_dataset = []\n",
    "    for n_file in range(0, len(list_dataset)): #for each .edf file in our list \n",
    "        dataset = read_raw_edf(list_dataset[n_file], preload=True) #load the file\n",
    "        list_load_dataset.append(dataset)\n",
    "        \n",
    "    return list_load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Function to exclude unused channnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below iterate thorugh each .edf file and remove the unwanted channels (i.e. the channels excluded from the include_channels list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excl_chan(data):\n",
    "\n",
    "    ''' \n",
    "    This function exclude the channels we don't need from further analysis. If you want\n",
    "    to add or remove some channels, modify the above list \"include_channels\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list_datasets: the list of datasets with unused channels removed\n",
    "    \n",
    "    '''\n",
    "       \n",
    "    list_datasets=[]     \n",
    "    for n_file in range(0, len(data)):    \n",
    "         for chan_name in data[n_file].ch_names: \n",
    "            if chan_name not in include_channels:\n",
    "                data[n_file].drop_channels([chan_name])   \n",
    "                list_datasets.append(data)\n",
    "                \n",
    "                                                   \n",
    "    return list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtering datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function iterates through each file and apply a Hamming windowed FIR band-pass filter (default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def filter_data(data):\n",
    "    \n",
    "    '''\n",
    "    This function filter the raw datasets. Because we are interested in low frequencies, \n",
    "    in the alpha-beta frequency range, we can band-pass filter between 1Hz-40Hz.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: our raw continuous unfiltered datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_data: the datasets containing the filtered data.\n",
    "    '''\n",
    "\n",
    "\n",
    "    filtered_data=[]\n",
    "    for file in range(0, len(data)):\n",
    "        data[file].filter(1., 30., fir_design='firwin') #apply band-pass filter between 1 and 40 HZ, our freqs range of interest\n",
    "        filtered_data.append(data[file])\n",
    "        \n",
    "    return filtered_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function to create epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to epoch the continuous filtered EEG datasets to segments with duration 6.998 seconds.\n",
    "With 30 sessions, we will have 300 epochs in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs(list_prep_dataset, duration):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function extract the epochs object from the preprocessed list of datasets. The conditions are:\n",
    "    Push = 1\n",
    "    Relax = 0\n",
    "    --> Note: This function should be called after the preprocessing, but before the ICA <--\n",
    "\n",
    "    :param list_prep_dataset: a list containing the preprocessed datasets\n",
    "    :param duration: the duration of each epochs (10 seconds)\n",
    "    :return: epochs: the mne.Epochs object containing the epoched data.\n",
    "    \"\"\"\n",
    "    \n",
    "    event_dict = {'Relax': 0, 'Push': 1}   \n",
    " \n",
    "    list_epochs = []  \n",
    "    for prep_dataset in list_prep_dataset:\n",
    "        events = mne.make_fixed_length_events(prep_dataset, id=0, start=65.0, stop=165.0, duration=duration) # make fixed-length events for each dataset in list_prep_dateset\n",
    "        \n",
    "        for n_events in range(0, len(events)):\n",
    "            if n_events % 2 == 1: \n",
    "                events[n_events][2] = 1 \n",
    "                \n",
    "          \n",
    "        # make epochs for each dataset\n",
    "        epochs = mne.Epochs(prep_dataset, events, tmin=0.0, tmax=9.998, event_id=event_dict, baseline=(0, 0), preload=True)\n",
    " \n",
    "        list_epochs.append(epochs)\n",
    "                \n",
    "        # combine epochs\n",
    "        epochs = mne.concatenate_epochs(list_epochs) \n",
    "\n",
    "        # crop start and end of the epochs based on provided time reference\n",
    "        epochs.crop(tmin=0.25, tmax=9.998 - 0.25) \n",
    "              \n",
    "        # Generate Standard montage (useful for ICA and TimeFrequency analyses)\n",
    "        biosemi_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "        epochs.set_montage(biosemi_montage) \n",
    "        \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function to plot Power Spectrum Density (PSD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot the PSD for each session to visually inspect them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):   #the input is, for example, our filtered dataset\n",
    "    for file in range(0, len(data)):\n",
    "        data[file].plot_psd(average=True)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Function to compute Morlet wavelets on epochs (to feed to the Common Spatial Pattern technique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function must be used only with the CSP function provided from MNE toolbox\n",
    "\n",
    "def epochs_power(data): #our input data are the epoched data\n",
    "    \n",
    "    #apply Morlet wavelets\n",
    "    freqs=np.ravel(freq_ranges)\n",
    "    pw = mne.time_frequency.tfr_morlet(data, freqs=freqs, n_cycles=n_cycles, use_fft=True, average=False,return_itc=False, decim=3, n_jobs=1)\n",
    "    n_col= pw.data.shape[3] #extract this dimension to use later \n",
    "    n_chan=pw.data.shape[1]\n",
    "    n_row= pw.data.shape[0]\n",
    "    \n",
    "    #average power in the freq band of interest\n",
    "    epo_pw = np.zeros(shape=(n_row,n_chan,n_col))  #initialise the variable with dimensions previously extracted\n",
    "    counter=0\n",
    "    \n",
    "    for samples in range (0,n_row):\n",
    "        for chan in range(0, n_chan): \n",
    "            select_pw = pw.data[samples][chan][(pw.freqs>fmin) & (pw.freqs<=fmax)][:]  \n",
    "            counter+=1\n",
    "            pw_avg = np.mean(select_pw, axis=0)  \n",
    "            epo_pw[samples,chan,:]=pw_avg\n",
    "    return epo_pw #the output is the average across all frequencies within each band, with dimensions (n_row,n_chan,n_col)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
