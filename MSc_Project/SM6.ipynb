{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID Number: 33385806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG based Brain-Computer Interface using Visual Imagery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Project for Computational Cognitive Neuroscience 2020/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding analysis using time-frequency features with SVM, LR, LDA, RF\n",
    "### In this notebook, features are extracted when noise and artifact removal are not applied. All 14-channels are selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture libraries   \n",
    "\n",
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install mne\n",
    "!{sys.executable} -m pip install mne-features\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import pathlib\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne import Epochs, create_info, events_from_annotations\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs,corrmap\n",
    "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch, tfr_stockwell,tfr_multitaper,tfr_array_morlet,AverageTFR\n",
    "from scipy import signal\n",
    "from scipy.integrate import simps\n",
    "matplotlib.use('Qt5Agg') #allow interactive plots\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import GeneralizingEstimator, Scaler,cross_val_multiscore, LinearModel, get_coef, Vectorizer, CSP, SlidingEstimator\n",
    "from mne.viz import centers_to_edges\n",
    "from mne.baseline import rescale\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support, precision_recall_curve, average_precision_score, plot_precision_recall_curve, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import plot_roc_curve, accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%run EEG_functions_1.ipynb import load_data, excl_chan, filter_data, make_epochs, plot_data, epochs_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the four classifiers when the features are extracted from non-preprocessed data (NO ICA and Bad Epochs Rejections performed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_data(os.getcwd()); #raw 30 sessions in .edf format will be uploaded\n",
    "\n",
    "include_channels = ['AF3','F7','F3','FC5','T7','P7','O1','O2','P8','T8','FC6','F4','F8','AF4']; \n",
    "excl_chan(raw_datasets) #remove the channels not included in the above list\n",
    "\n",
    "filter_data(raw_datasets) #filter between 1-30 Hz\n",
    "\n",
    "epoched_data=make_epochs(raw_datasets, 10)  #create epochs\n",
    "\n",
    "print(epoched_data.get_data().shape) #the final shape is n_epochs, chans, samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the same procedure, extract features from unpreprocessed data\n",
    "\n",
    "freqs = np.logspace(*np.log10([2, 30]), num=40) # define frequencies of interest (log-spaced) \n",
    "n_cycles = freqs / 2.  # different number of cycle per frequency\n",
    "\n",
    "power_dec = mne.time_frequency.tfr_morlet(epoched_data, freqs=freqs, n_cycles=n_cycles, use_fft=True, average=False,\n",
    "                                           return_itc=False, decim=3, n_jobs=1)  \n",
    " \n",
    "\n",
    "print(power_dec.data.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in Theta band 4-7Hz\n",
    "\n",
    "n_col= power_dec.data.shape[3] #extract n columns from pw output\n",
    "n_chan=power_dec.data.shape[1] #extract n channels\n",
    "n_row= power_dec.data.shape[0] #extract n rows\n",
    "\n",
    "\n",
    "theta_pow_dec = np.zeros(shape=(n_row,n_chan,n_col))  \n",
    "counter=0\n",
    "for samples in range (0,n_row):\n",
    "    for chan in range(0, n_chan):\n",
    "        pow_t = power_dec.data[samples][chan][(power_dec.freqs>=4) & (power_dec.freqs<7)][:]  \n",
    "        counter+=1\n",
    "        pow_avg_t = np.mean(pow_t, axis=0) \n",
    "        theta_pow_dec[samples,chan,:]=pow_avg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Alpha band 8-12Hz\n",
    "\n",
    "\n",
    "n_col= power_dec.data.shape[3] #extract n columns from pw output\n",
    "n_chan=power_dec.data.shape[1] #extract n channels\n",
    "n_row= power_dec.data.shape[0] #extract n rows\n",
    "\n",
    "alpha_pow_dec = np.zeros(shape=(n_row,n_chan,n_col))   \n",
    "counter=0\n",
    "for samples in range (0,n_row):\n",
    "    for chan in range(0, n_chan): \n",
    "        pow_a = power_dec.data[samples][chan][(power_dec.freqs>=8) & (power_dec.freqs<12)][:]  \n",
    "        counter+=1\n",
    "        pow_avg_a = np.mean(pow_a, axis=0)  \n",
    "        alpha_pow_dec[samples,chan,:]=pow_avg_a  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beta band\n",
    "\n",
    "n_col= power_dec.data.shape[3] #extract n columns from pw output\n",
    "n_chan=power_dec.data.shape[1] #extract n channels\n",
    "n_row= power_dec.data.shape[0] #extract n rows\n",
    "\n",
    "beta_pow_dec = np.zeros(shape=(n_row,n_chan,n_col))    \n",
    "counter=0\n",
    "for samples in range (0,n_row):\n",
    "    for chan in range(0, n_chan): \n",
    "        pow_b = power_dec.data[samples][chan][(power_dec.freqs>=13) & (power_dec.freqs<30)][:]  \n",
    "        counter+=1\n",
    "        pow_avg_b = np.mean(pow_b, axis=0)  \n",
    "        beta_pow_dec[samples,chan,:]=pow_avg_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, initialise the below variables to store the accuracies from all classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, f1_scores = [], [] #run this cell only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= alpha_pow_dec  # theta_pow_dec, alpha_pow_dec, beta_pow_dec\n",
    "labels = epoched_data.events[:,-1] \n",
    "train_data, test_data, labels_train, labels_test = train_test_split(data, labels, test_size=0.3, random_state=173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_pip = make_pipeline(Vectorizer(), StandardScaler(), svm.SVC(probability=True))  #define pipeline\n",
    "parameters = {'svc__kernel':['rbf', 'sigmoid'], 'svc__C':[0.1, 1, 10], 'svc__gamma':[0.1,0.01,0.001]}\n",
    "#Apply GridSearchCV to identify the best parameters\n",
    "gs_cv_svm = GridSearchCV(clf_svm_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=10), return_train_score=True) \n",
    "\n",
    "#Training is done by passing the training data and their labels to fit() function.\n",
    "\n",
    "gs_cv_svm.fit(train_data, labels_train)\n",
    "print('Best Parameters: {}'.format(gs_cv_svm.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_svm.best_score_))\n",
    "\n",
    "\n",
    "predictions_svm = gs_cv_svm.predict(test_data)\n",
    "\n",
    "#Evaluate\n",
    "report_svm = classification_report(labels_test, predictions_svm, target_names=['Relax', 'Push'])\n",
    "print('SVM Clasification Report:\\n {}'.format(report_svm))\n",
    "\n",
    "acc_svm = accuracy_score(labels_test, predictions_svm)\n",
    "print(\"Accuracy of SVM model: {}\".format(acc_svm))\n",
    "\n",
    "precision_svm,recall_svm,fscore_svm,support_svm=precision_recall_fscore_support(labels_test,predictions_svm,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_svm,recall_svm,fscore_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under Curve (AUC) value\n",
    "auc = roc_auc_score(labels_test, predictions_svm)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC result\n",
    "svm_roc = plot_roc_curve(gs_cv_svm, test_data, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall result\n",
    "svm_pr = plot_precision_recall_curve(gs_cv_svm, test_data, labels_test) #precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "errors_svc = abs(predictions_svm - labels_test)\n",
    "print('Average absolute error:', round(np.mean(errors_svc), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis())\n",
    "parameters = {'lineardiscriminantanalysis__solver':['svd']}\n",
    "gs_cv_lda = GridSearchCV(clf_lda_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=10), return_train_score=True) \n",
    "gs_cv_lda.fit(train_data,labels_train)\n",
    " \n",
    "print('Best Parameters: {}'.format(gs_cv_lda.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_lda.best_score_))\n",
    "\n",
    "\n",
    "#Predictions\n",
    "predictions_lda = gs_cv_lda.predict(test_data)\n",
    "\n",
    "#Evaluation\n",
    "report_lda = classification_report(labels_test, predictions_lda, target_names=['Relax', 'Push'])\n",
    "print('LDA Clasification Report:\\n {}'.format(report_lda))\n",
    "\n",
    "acc_lda = accuracy_score(labels_test, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test,predictions_lda,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under Curve (AUC) value\n",
    "auc = roc_auc_score(labels_test, predictions_lda)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC \n",
    "lda_roc = plot_roc_curve(gs_cv_lda, test_data, labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall \n",
    "lda_pr = plot_precision_recall_curve(gs_cv_lda,test_data, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "errors_lda = abs(predictions_lda - labels_test)\n",
    "print('Average absolute error:', round(np.mean(errors_lda), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(max_iter=5000))\n",
    "parameters ={'logisticregression__C': np.logspace(0, 4, 100)}  \n",
    "\n",
    "\n",
    "gs_cv_lr = GridSearchCV(clf_lr_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=10))\n",
    "gs_cv_lr.fit(train_data, labels_train)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_lr.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_lr.best_score_))\n",
    "\n",
    "#Predictions\n",
    "predictions_lr = gs_cv_lr.predict(test_data)\n",
    "\n",
    "#Evaluation\n",
    "report_lr = classification_report(labels_test, predictions_lr, target_names=['Relax', 'Push'])\n",
    "print('LR Clasification Report:\\n {}'.format(report_lr))\n",
    "\n",
    "acc_lr = accuracy_score(labels_test, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lr=precision_recall_fscore_support(labels_test,predictions_lr,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under Curve (AUC) value\n",
    "auc = roc_auc_score(labels_test, predictions_lr)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC\n",
    "lr_roc = plot_roc_curve(gs_cv_lr, test_data, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "lr_pr = plot_precision_recall_curve(gs_cv_lr, test_data, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "errors_lr = abs(predictions_lr - labels_test)\n",
    "print('Average absolute error:', round(np.mean(errors_lr), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_pip = make_pipeline(Vectorizer(), StandardScaler(), RandomForestClassifier()) \n",
    "parameters = {'randomforestclassifier__n_estimators':[100,200,300,400,500,600,700], 'randomforestclassifier__criterion':['gini','entropy'], 'randomforestclassifier__max_depth':[1,2,3,4,5]} \n",
    "gs_cv_rf = GridSearchCV(clf_rf_pip, parameters, scoring='accuracy', cv=StratifiedKFold(n_splits=10), return_train_score=True)  \n",
    "gs_cv_rf.fit(train_data,labels_train)\n",
    "\n",
    "print('Best Parameters: {}'.format(gs_cv_rf.best_params_))\n",
    "print('Best Score: {}'.format(gs_cv_rf.best_score_))\n",
    "\n",
    "predictions_rf = gs_cv_rf.predict(test_data)\n",
    "\n",
    "#Evaluation\n",
    "report_rf = classification_report(labels_test, predictions_rf, target_names=['Relax', 'Push'])\n",
    "print('RF Clasification Report:\\n {}'.format(report_rf))\n",
    "\n",
    "acc_rf = accuracy_score(labels_test, predictions_rf)\n",
    "print(\"Accuracy of RF model: {}\".format(acc_rf))\n",
    "\n",
    "precision_rf,recall_rf,fscore_rf,support_rf=precision_recall_fscore_support(labels_test,predictions_rf,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_rf,recall_rf,fscore_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under Curve (AUC) value\n",
    "auc = roc_auc_score(labels_test, predictions_rf)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC\n",
    "rf_roc = plot_roc_curve(gs_cv_rf, test_data, labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "rf_pr = plot_precision_recall_curve(gs_cv_rf, test_data, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "errors_rf = abs(predictions_rf - labels_test)\n",
    "print('Average absolute error:', round(np.mean(errors_rf), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.append([acc_svm, acc_lda, acc_lr, acc_rf])\n",
    "f1_scores.append([fscore_svm, fscore_lda, fscore_lr, fscore_rf ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "svm_roc.plot(ax=ax, alpha=0.8,label='SVM')\n",
    "lda_roc.plot(ax=ax, alpha=0.8,label='LDA')        \n",
    "lr_roc.plot(ax=ax, alpha=0.8,label='LR')   \n",
    "rf_roc.plot(ax=ax, alpha=0.8,label='RF')\n",
    "\n",
    " \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "svm_pr.plot(ax=ax, alpha=0.8,label='SVM')\n",
    "lda_pr.plot(ax=ax, alpha=0.8,label='LDA')\n",
    "lr_pr.plot(ax=ax, alpha=0.8,label='LR')\n",
    "rf_pr.plot(ax=ax, alpha=0.8,label='RF')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance of these four different models (Accuracy & F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(accuracies)) #the final shape should be (3,4)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy scores\n",
    "# Choose the height of the bars\n",
    "barWidth = 0.2\n",
    "\n",
    "\n",
    "bars1 = [row[0] for row in accuracies ]  #svm\n",
    "bars2 = [row[1] for row in accuracies ]  #lda\n",
    "bars3 = [row[2] for row in accuracies ]  #lr\n",
    "bars4 = [row[3] for row in accuracies ]  #rf\n",
    " \n",
    "\n",
    "\n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "\n",
    "\n",
    "# Create the bars\n",
    "ax = plt.axes()\n",
    "plt.bar(r1, bars1, color='#87CEFA', width=barWidth, edgecolor='white', label='SVM')\n",
    "plt.bar(r2, bars2, color='#FFE4E1', width=barWidth, edgecolor='white', label='LDA')\n",
    "plt.bar(r3, bars3, color='#CD5C5C', width=barWidth, edgecolor='white', label='LR')\n",
    "plt.bar(r4, bars4, color='#C5E384', width=barWidth, edgecolor='white', label='RF')\n",
    "\n",
    "plt.axhline(y=0.5, color='k', linestyle='--',linewidth=0.4)\n",
    "#plt.axhline(y=0.6, color='r', linestyle='--',linewidth=0.4)\n",
    "plt.xlabel('Classification Tasks')\n",
    "plt.ylabel(' Accuracies')\n",
    "ax.set_xticks([0.3,1.3,2.3])\n",
    "ax.set_xticklabels(['Theta','Alpha','Beta'])\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(0.98, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1-scores\n",
    "# Choose the height of the bars\n",
    "barWidth = 0.2\n",
    "\n",
    "\n",
    "bars1 = [row[0] for row in f1_scores ] #svm\n",
    "bars2 = [row[1] for row in f1_scores ] #lda\n",
    "bars3 = [row[2] for row in f1_scores ] #lr\n",
    "bars4 = [row[3] for row in f1_scores ] #rf\n",
    " \n",
    "\n",
    "# The x position of bars\n",
    "\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "\n",
    "\n",
    "# Create the bars\n",
    "ax = plt.axes()\n",
    "plt.bar(r1, bars1, color='#87CEFA', width=barWidth, edgecolor='white', label='SVM')\n",
    "plt.bar(r2, bars2, color='#FFE4E1', width=barWidth, edgecolor='white', label='LDA')\n",
    "plt.bar(r3, bars3, color='#CD5C5C', width=barWidth, edgecolor='white', label='LR')\n",
    "plt.bar(r4, bars4, color='#C5E384', width=barWidth, edgecolor='white', label='RF')\n",
    "\n",
    "plt.axhline(y=0.5, color='k', linestyle='--',linewidth=0.4)\n",
    "#plt.axhline(y=0.6, color='r', linestyle='--',linewidth=0.4)\n",
    "plt.xlabel('Classification Tasks')\n",
    "plt.ylabel(' F1-scores')\n",
    "ax.set_xticks([0.3,1.3,2.3])\n",
    "ax.set_xticklabels(['Theta','Alpha','Beta'])\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(0.98, 1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
